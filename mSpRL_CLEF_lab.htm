<html>
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=iso-8859-1">
    <meta name="GENERATOR" content="Mozilla/4.5 [en] (Win95; I) [Netscape]">
     <link rel="shortcut icon" href="http://tulane.edu/sites/tulane/files/favicon_0.ico" type="image/vnd.microsoft.icon" /> 
    <title>CLEF Lab on Multimodal Spatial Role Labeling</title>
    <X-SAS-WINDOW TOP=78 BOTTOM=636 LEFT=30 RIGHT=560/>
    <style type="text/css">code{white-space: pre;}</style>
  <link rel="stylesheet" href="parisa.css" type="text/css" />
  <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_CHTML-full" type="text/javascript"></script>
    <link href='https://fonts.googleapis.com/css?family=EB+Garamond' rel='stylesheet' type='text/css'>
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({ TeX: { equationNumbers: {autoNumber: "all"} } });
    </script>
  </head>

  <body bgcolor="#FFFFFF">
  <center>
  <h1>
	<font size=+3> Multimodal Spatial Role Labeling </font>
  </h1>
 <h3><font size=+2><a href="http://clef2017.clef-initiative.eu/">CLEF-2017 Lab</a>, Dublin, 11-14 September 2017</font></h3></center>
 <img style="width: 140px; height: auto; margin-left:70px;margin-top: 40px;" src="images/logo.jpg" align="left">
<img style="margin-right:180px; margin-top: 40px;" src="images/clefDublin2017.png" align="right">


<p style="margin-left: 60px; margin-right: 250px; margin-top: 50px; text-align: justify;">
<font size=+1>
 The main goal of multi-modal SpRL is to explore the extraction of spatial information from two information resources that are image and text. 
 This is important for various applications such as semantic search, 
  question answering, geographical information systems and even in robotics for machine understanding of navigational instructions or instructions for grabbing and manipulating objects.
 It is also essential for some specific tasks such as text to scene conversion or vice-versa, scene understanding as well as general information retrieval tasks when using a huge amount of available multimodal data from various resources. Moreover, there is an increasing interest in the extraction of spatial information from medical images that are accompanied by natural language descriptions. 
 <strong>In this workshop we accept papers related to this research topic, moreover, we provide a pilot task and data for the interested participants to build a system for such extractions. 
</strong>
</font>

 </p>
 

<hr>

<div id="sidebar">
    <p>
    <a href="mSpRL_CLEF_lab.htm#news">News</a>
    </p>
    <p>
    <a href="mSpRL_CLEF_lab.htm#tasks">Tasks</a>
    </p>
    <p>
    <a href="mSpRL_CLEF_lab.htm#evaluation">Evaluation</a>
    </p>
    <p>
    <a href="mSpRL_CLEF_lab.htm#data">Data</a>
    </p>
    <p>
    <a href="mSpRL_CLEF_lab.htm#dates">Important dates</a>
    </p>
    <p>
    <a href="mSpRL_CLEF_lab.htm#organizers">Organizers</a>
    </p>
     <p>
    <a href="mSpRL_CLEF_lab.htm#references">References</a>
    </p>
    
    <br>
</div>


<div id="body_outer">
<div id="body">
<div id="header">

<h2 id="news">News</h2>
<ul>
<li> <p style="margin-left: 5px; margin-right: 30px; text-align: justify;">
 <strong> 09/07/2016.</strong> The lab proposal accepted. </p></li>
 <li> <strong> 11/10/2016. </strong> The Registration is open, <a href="http://clef2017-labs-registration.dei.unipd.it/registrationForm.php">click here to register</a>. <br>
     <a class="twitter-timeline" data-width="750" data-height="150" href="https://twitter.com/mSpRL2017">Tweets by mSpRL2017</a>
    <script async src="//platform.twitter.com/widgets.js" charset="utf-8"></script>
    </p>
  
 </li>
</ul>
<h2 id="labSetting">Lab setting</h2>
The workshop has two types of submissions, research papers and shared task  participant papers. The main topics for the general research papers include the following topics and the related areas :
<ul>
<li> Extraction of spatial information form text.
<li> Extraction of spatial information form images.
<li> Combining various modalities for information extraction.
<li> Combining modalities to expand the feature space for machine learning models.
<li> Combining modalities to help resolving inconsistencies and ambiguity in the extractions.
<li> Spatial representations appropriate for information extraction from image and text.
</ul>
<h2 id="tasks">Pilot Task Description</h2>

<strong>Multi-modal spatial role labeling </strong>is an extension of the <strong>spatial role labeling </strong>task to exploit multi-modal data including image and text for extraction of spatial information. The task of <a href="http://www.cs.tulane.edu/~pkordjam/SpRL.htm">spatial role labeling (SpRL)</a>[1] formalizes the representation of spatial concepts and relations in the natural language text to be mapped to qualitative spatial representation models by means of machine learning techniques. Let us illustrate the spatial role labeling task with the following example: 

<p style="margin-left: 30px; margin-right: 30px; text-align: center;border:3px; border-style:solid; border-color:green; padding: 1em;"><font color=blue>
<i>About 20 kids in traditional clothing and hats waiting on stairs. A house and a green wall with gate in the background. A sign saying that plants can't be picked up on the right. </i>
</p></font>

<ol>

<li>
The first level of this task is to extract spatial roles including, 
<ul>
<li><strong>Spatial indicator</strong> : Spatial indicators are triggers that indicate the existence of spatial information in a sentence. </li>
<li><strong>Trajector</strong> : a Trajector is an entity whose location is described </li>
<li><strong>Landmark</strong>: a Landmark is a reference object for describing the location of a trajector.</li>
</ul> 

<br>
<p style="border:3px; border-style:solid; border-color:green; padding: 1em;"><font color=blue>
<i> About 20 </i> [<b><i>kids</i></b>]<sub>TRAJECTOR </sub> in traditional clothing and hats waiting [<b><i>on</i></b>]<sub>SPATIAL_INDICATOR</sub> [<b><i>stairs</i></b>]<sub>LANDMARK</sub>.
</font></p>
For example, in the above sentences the location of <strong>kids </strong>that is the {trajector} has been described with respect to the {stairs} that is the {landmark} using the preposition {on} that is the {spatial indicator}.  These are examples of some spatial roles that we aim to extract from the sentence. 

<br>

Figure[1], shows the picture related to the language descriptions. Extraction of the objects from the image might help the extraction of trajector and landmark from sentences. 

</li>

<li>
The second level of this task is to extract spatial relationships. In the same example we have the triplet
 <p style="border:3px; border-style:solid; border-color:green; padding: 1em;"><font color=blue>spatial_relation(kids, on, stairs)</font></p>
 that form a kind of spatial relation/link between the three above mentioned roles. Recognizing the spatial relations is very challenging because there could be several spatial roles in the sentence and the model should be able to recognize the right connections. For example (waiting, on, stairs) is a wrong relation here because "kids" is the trajector in this sentence not "waiting". For this level also the information from the image can be very helpful. 
</li>
<li>
The third level of the task is to recognize the type of spatial relations. The type of relations is expressed in terms of topological models, according to the well-known <strong>RCC (regional connection calculus)</strong> qualitative representation, in addition to 6 <strong>directional</strong> relations. RCC5 includes (Externally connected, Disconnected, Overlapping, Proper part, and Equality). Directional includes relative directions such as left, right, above, below, behind, front. In the above example we can state the type of relation between 
the roles in the triplet as 
<p style="border:3px; border-style:solid; border-color:green; padding: 1em;"><font color=blue>above(kids, stairs)</font></p> 
</li>
</ol>  

<figure>
    <img src="CLEFImage.JPG" alt="Example picture" width= "450px" height= "auto" align="center"/>
    <figcaption>Fig 1. About 20 kids in traditional clothing and hats waiting on stairs. A house and a green wall with gate in the background. A sign saying that plants can't be picked up on the right.</figcaption>
</figure>

The data has been annotated with the above mentioned roles and relations and the annotated data can be used to train machine leaning models to do this kind of extractions automatically. Since doing this task has been appeared to be very challenging due to the ambiguity and polysemy in the text[2], we have decided to exploit the <strong>images</strong> that come along with CLEF textual data to improve the recognition of the spatial objects and their relations. Specifically, we believe the images will certainly help to improve the recognition of the type of relations given that the geometrical features of the boundaries of the objects in the images are closer to the formal qualitative representations of the relationships compared to the counterpart linguistic descriptions.

<h2 id="data">Data</h2>

The dataset we prepared for this task has been based on CLEF, IAPRTC-12 Image Benchmark. The original data includes touristic pictures along with textual description of the pictures. We have annotated the texts of a subset of images according to spatial role labeling annotation scheme[1]. 
<ul>
<li> <a href="data/mSpRL2017Training.zip"> Download mSpRL-2017 Training data</a> (published 11/19/2016)</li>
<li>
Download mSpRL-2017 Testing data (not published yet)
</li>
</ul>
<h2 id="evaluation">Evaluation</h2>
We split the human annotated data into a training set and a testing set. The extractions of the model are compared to human annotations. The evaluation metrics are precision, recall and F1 measures. These metrics are used to evaluate the performance on recognizing each type of role and each type of relation separately. 

<h2 id="baselines">Baselines</h2>
 The baseline models have been implemented in <a href="https://github.com/IllinoisCogComp/saul/blob/master/saul-core/README.md"><strong>Saul</strong></a> which has been equipped by various NLP tools for extraction of linguistic features.  This is a very flexible platform to facilitate programming and designing various models for the interested participants and it is publicly available. The <a href="https://github.com/IllinoisCogComp/saul/blob/master/saul-examples/src/main/scala/edu/illinois/cs/cogcomp/saulexamples/nlp/SpatialRoleLabeling/README.md"><strong>SpRL models </strong></a> and various number of features have been implemented based on the previous related works.  


<h2 id="dates">Important dates</h2>
<ul>
<li>Labs registration opens: 4 November 2016
<li>Registration closes: 21 April 2017
<li>End Evaluation Cycle: 5 May 2017
<li>Submission of Participant Papers [CEUR-WS]: 26 May 2017
<li>Review process of participant papers: 26 May-16 June 2017
<li>Submission of Lab Overviews [LNCS]: 9 June 2017
<li>Notification of Acceptance Lab Overviews [LNCS]: 16 June 2017
<li>Camera Ready Copy of Lab Overviews [LNCS] due: 23 June 2017
<li>Notification of Acceptance Participant Papers [CEUR-WS]: 16 June 2017
<li>Camera Ready Copy of Participant Papers and Extended Lab Overviews [CEUR-WS] due: 3 July 2017
<li>CEUR-WS Working Notes Preview for checking: 21 July 2017
<li>Feedback from lab organizers/participants to correct any error [NO CHANGES/UPDATES ALLOWED]: 28 July 2017
 </ul>

<h2 id="organizers">Organizers</h2>

<ul>
<li><p style="margin-left: 0px; text-align: justify;"><strong><a href="http://www.cs.tulane.edu/~pkordjam/index.htm" >Parisa Kordjamshidi</a>,</strong> Tulane University, pkordjam@tulane.edu</p></li>

<li><p style="margin-left: 0px; text-align: justify;"><strong>Taher Rahgooy,</strong> Bu-Ali Sina University, taher.rahgooy@gmail.com</p></li>

<li><p style="margin-left: 0px; text-align: justify;"><a href="https://people.cs.kuleuven.be/~sien.moens/"><strong>Marie-Francine Moens,</strong> </a>KULeuven, sien.moens@cs.kuleuven.be</p></li>

<li><p style="margin-left: 0px; text-align: justify;"><a href="http://jamespusto.com"><strong>James Pustejovsky</strong></a>, Brandeis University, jamesp@cs.brandeis.edu </p></li>

<li><strong>Oswaldo Ludwig</strong>, oswaldoludwig@gmail.com</li>
<li><p style="margin-left: 0px; text-align: justify;"><strong>Kirk Roberts</strong>, UT Health Science Center at Houston, kirk.roberts@uth.tmc.edu</p></li>
</ul>
<a href="mailto:sprl_lab_clef2017@googlegroups.com?Subject=mSpRL%20CLEF-2017" target="_top">Send Mail</a>
</p>

<h2 id="references">References</h2>

<p style="margin-left: 0px; text-align: justify;">[1]. Kordjamshidi, P., Moens, M., van Otterlo, M., (2010). Spatial Role Labeling: Task Definition and Annotation Scheme, LREC'10. 
</p>
<p style="margin-left: 0px; text-align: justify;">[2]. Kordjamshidi, P., van Otterlo, M., Moens, M. (2011). Spatial role labeling: Towards extraction of spatial relations from natural language. ACM Transactions on Speech and Language Processing, 8(3), 4-36.
</p>
<p style="margin-left: 0px; text-align: justify;">[3]. Kordjamshidi, P., Moens, M. (2015). Global machine learning for spatial ontology population. Journal of Web Semantics, 30, 3-21. <a href="papers/webSemanticJournalPaperPreprint.pdf" target="_blank">Download</a></p>

<p style="margin-left: 0px; text-align: justify;">[4]. Kordjamshidi,
P., van Otterlo, M., Moens, M. (2015). Spatial role labeling annotation scheme. In: Pustejovsky J., Ide N. (Eds.),
Handbook of Linguistic Annotation Springer Verlag. <a href="papers/SpatialRoleLabeling_MinorReview">Download</a></p>

<p style="margin-left: 0px; text-align: justify;">[5]. James Pustejovsky, Jessica Moszkowicz, and Marc Verhagen. A linguistically grounded annotation language for spatial information. TAL, 53(2), 2012. 6
</p>
<p style="margin-left: 0px; text-align: justify;">[6]. James Pustejovsky and Zachary Yocum. Capturing motion in iso-spacebank. In Workshop on Interoperable Semantic Annotation, page 25, 2013.
</p>
<br>
<img style="width: 250px; height: auto; margin-bottom:50px" alt="Cost" src="images/logo_cost.png" align="right"/>
<img style="width: 250px; height: auto; margin-bottom:50px" alt="CLEF initiative" src="images/clef-initiative-logo-full.png" align="left"/>

<!-- Start of StatCounter Code for Default Guide -->
<script type="text/javascript">
var sc_project=11083511; 
var sc_invisible=1; 
var sc_security="2f97c6cf"; 
var scJsHost = (("https:" == document.location.protocol) ?
"https://secure." : "http://www.");
document.write("<sc"+"ript type='text/javascript' src='" +
scJsHost+
"statcounter.com/counter/counter.js'></"+"script>");
</script>
<noscript><div class="statcounter"><a title="web analytics"
href="http://statcounter.com/" target="_blank"><img
class="statcounter"
src="//c.statcounter.com/11083511/0/2f97c6cf/1/" alt="web
analytics"></a></div></noscript>
<!-- End of StatCounter Code for Default Guide -->

  </body>
</html>